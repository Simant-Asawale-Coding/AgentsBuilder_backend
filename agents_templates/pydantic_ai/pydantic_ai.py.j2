import os
from dotenv import load_dotenv
import asyncio
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.azure import AzureProvider
from pydantic_ai.mcp import MCPServerHTTP
import uvicorn

load_dotenv()

app = FastAPI()

from typing import List, Optional
import logging
logging.basicConfig(level=logging.INFO)

class ChatRequest(BaseModel):
    input: str
    chat_history: Optional[List[str]] = None  # List of previous messages (user/assistant turns)

def get_azure_llm():
    return OpenAIModel(
        "{{ llm_model }}",
        provider=AzureProvider(
            azure_endpoint="{{ llm_endpoint }}",
            api_version="{{ llm_api_version }}",
            api_key="{{ llm_api_key }}",
        ),
    )

# --- MCP Tool Config ---
ALL_TOOLS = [
    {% for mcp in mcp_servers %}
    {"id": "{{ mcp.name }}", "desc": "{{ mcp.desc or mcp.name }}", "url": "{{ mcp.url }}"},
    {% endfor %}
]
MCP_TOOL_CONFIGS = ALL_TOOLS

mcp_tool_status = {tool["id"]: False for tool in MCP_TOOL_CONFIGS}
mcp_tool_instances = {tool["id"]: None for tool in MCP_TOOL_CONFIGS}

async def check_tool_health(tool_id, url):
    old_tool = mcp_tool_instances.get(tool_id)
    tool = MCPServerHTTP(url=url)
    try:
        await tool.__aenter__()
        mcp_tool_status[tool_id] = True
        mcp_tool_instances[tool_id] = tool
        logging.info(f"Tool {tool_id} is available.")
        if old_tool and old_tool is not tool:
            try:
                await old_tool.__aexit__(None, None, None)
            except Exception as cleanup_err:
                logging.warning(f"Error closing old tool {tool_id}: {cleanup_err}")
    except Exception as e:
        mcp_tool_status[tool_id] = False
        if old_tool:
            try:
                await old_tool.__aexit__(None, None, None)
            except Exception as cleanup_err:
                logging.warning(f"Error closing tool {tool_id}: {cleanup_err}")
        mcp_tool_instances[tool_id] = None
        logging.warning(f"Tool {tool_id} unavailable: {e}")

async def background_health_checker():
    while True:
        try:
            tasks = [check_tool_health(tool["id"], tool["url"]) for tool in MCP_TOOL_CONFIGS]
            await asyncio.gather(*tasks)
        except Exception as loop_err:
            logging.error(f"Health checker loop error: {loop_err}")
        await asyncio.sleep(30)

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(background_health_checker())
    await asyncio.sleep(1)  # Give checker a moment to run on startup

def build_system_message(available_tool_names, user_system_message):
    all_tool_list = ', '.join([f"{tool['name']} ({tool['url']})" for tool in ALL_TOOLS])
    available_list = ', '.join([tool['name'] for tool in ALL_TOOLS if tool['name'] in available_tool_names])
    # Compose the robust system message
    return (
        ("SYSTEM_MESSAGE_BY_CREATOR:" + user_system_message.strip() + "\n\n") +
        f"DEVELOPER_NOTE:You are a helpful Dynamically created Pydantic AI MCP(Model Context Protocol) tools agent created through a portal - Vartik MCP Agents Studio. MCP allows users to connect to their tools in a secure and efficient manner.\nPydantic AI brings the strengths of the Pydantic data validation library to AI agent development, emphasizing schema-first design and strict input/output validation. With Pydantic AI, you define exactly what data your agent should receive and produce, ensuring outputs conform to your specified formatsâ€”reducing errors and increasing reliability in production. It is particularly effective for use cases requiring structured outputs, type safety, and predictable agent behavior. Pydantic AI allows you to chain steps and even create graph-based flows, but its primary focus is on single-agent workflows with strong data validation. While it may require more setup and a good understanding of Pydantic models, it is an excellent choice for applications where data integrity and robust schema enforcement are critical.\n\n----------------------------------------------------------------\n\n-U need to understand that u will be serving requests directly to the end user, so answer quetions accordingly.\n-Always keep your tone professional and appropriate, unless user specifically asks for a different tone that's not inappropriate.\n-Always keep ur answer very short, on point and concise, unless the SYSTEM_MESSAGE_BY_CREATOR or the user query mentions otherwise.\n-You were assigned the given tools: {all_tool_list}.\n"
        f"Currently, the following tools are available: {available_list}.\n"
        "If the user asks for a tool that is not available, inform them that the tool is down and might be under maintenance and list the available tools."
    )

async def get_current_agent():
    llm = get_azure_llm()
    available_tools = [mcp_tool_instances[tool['id']] for tool in MCP_TOOL_CONFIGS if mcp_tool_status[tool['id']] and mcp_tool_instances[tool['id']] is not None]
    available_tool_ids = [tool['id'] for tool in MCP_TOOL_CONFIGS if mcp_tool_status[tool['id']] and mcp_tool_instances[tool['id']] is not None]
    system_prompt = build_system_prompt(available_tool_ids)
    return Agent(llm, mcp_servers=available_tools, system_prompt=system_prompt)

@app.post("/chat")
async def chat(request: ChatRequest):
    # Format chat history as a natural conversation transcript
    chat_history = request.chat_history or []
    conversation = []
    for msg in chat_history:
        msg_strip = msg.strip()
        if msg_strip.lower().startswith("user:"):
            conversation.append(f"User: {msg_strip[5:].strip()}")
        elif msg_strip.lower().startswith("assistant:"):
            conversation.append(f"Assistant: {msg_strip[10:].strip()}")
        else:
            conversation.append(msg_strip)
    conversation.append(f"User: {request.input.strip()}")
    final_query = "Conversation so far:\n" + "\n".join(conversation)
    try:
        agent = await get_current_agent()
        result = await agent.run(final_query)
        return {"output": result.output}
    except Exception as e:
        logging.error(f"Error during agent run: {e}")
        return {"output": "An error occurred while running the agent. Please try again later and make sure the prompt follows safety guidelines."}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=80)
